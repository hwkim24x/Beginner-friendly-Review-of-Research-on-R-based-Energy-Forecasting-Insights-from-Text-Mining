{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88c4d17",
   "metadata": {},
   "source": [
    "# 1. PDF to Text Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efffbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "# --- 1. Path Configuration ---\n",
    "# Get the current working directory, which is the 'src' folder.\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Move up one level to the project's root directory.\n",
    "base_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Define the path to the data folder, which is expected to contain PDF files.\n",
    "data_folder_path = os.path.join(base_dir, 'data')\n",
    "\n",
    "# Define the input directory containing PDF files.\n",
    "pdf_folder_path = os.path.join(data_folder_path, 'pdf')\n",
    "\n",
    "# Define the output directory where the extracted text files will be saved.\n",
    "txt_save_path = os.path.join(data_folder_path, 'txt')\n",
    "\n",
    "# --- 2. Directory Setup ---\n",
    "# Check if the output directory exists. If not, create it to avoid errors.\n",
    "if not os.path.exists(txt_save_path):\n",
    "    os.makedirs(txt_save_path)\n",
    "    print(f\"Created a new folder: '{txt_save_path}'.\")\n",
    "\n",
    "# --- 3. PDF File Handling ---\n",
    "# Get a list of all files in the specified PDF folder.\n",
    "# This part includes error handling for cases where the folder path is invalid.\n",
    "try:\n",
    "    file_list = os.listdir(pdf_folder_path)\n",
    "    # Filter the list to include only files with a '.pdf' extension.\n",
    "    pdf_files = [f for f in file_list if f.endswith('.pdf')]\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: PDF folder path not found. Please check the path: '{pdf_folder_path}'.\")\n",
    "    pdf_files = []\n",
    "\n",
    "# --- 4. Main Processing Loop ---\n",
    "# Iterate through each identified PDF file to perform text extraction.\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(pdf_folder_path, pdf_file)\n",
    "    # Create the corresponding output filename with a '.txt' extension.\n",
    "    txt_path = os.path.join(txt_save_path, os.path.splitext(pdf_file)[0] + '.txt')\n",
    "\n",
    "    try:\n",
    "        # Open the PDF file in binary read mode ('rb').\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = ''\n",
    "            \n",
    "            # Loop through every page in the PDF and concatenate the extracted text.\n",
    "            for page in reader.pages:\n",
    "                # Add extracted text. Use 'or '' ' to handle pages that may not have text.\n",
    "                text += page.extract_text() or ''\n",
    "            \n",
    "            # Write the final extracted text to a new TXT file.\n",
    "            with open(txt_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(text)\n",
    "        \n",
    "        print(f\"Text successfully extracted from '{pdf_file}' and saved to '{os.path.basename(txt_path)}'.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Print an error message if any issue occurs during the process for a specific file.\n",
    "        print(f\"An error occurred while processing the file '{pdf_file}': {e}\")\n",
    "\n",
    "# --- 5. Completion Message ---\n",
    "print(\"\\nAll PDF files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6381d5",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a888e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# --- 1. Path Configuration ---\n",
    "# Get the current working directory, which is the 'src' folder.\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Move up one level to the project's root directory.\n",
    "base_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Define the path to the 'data' folder from the project's root.\n",
    "data_folder_path = os.path.join(base_dir, 'data')\n",
    "\n",
    "# Path to the custom stopwords file.\n",
    "custom_stopwords_path = os.path.join(data_folder_path, 'stop_words_english.txt')\n",
    "\n",
    "# Set the input and output directory paths for processing.\n",
    "input_directory = os.path.join(data_folder_path, 'txt')\n",
    "output_directory = os.path.join(data_folder_path, 'txt2')\n",
    "\n",
    "# --- 2. NLTK Setup ---\n",
    "# This script requires the following NLTK resources.\n",
    "# Uncomment and run these lines once to download the necessary data.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# --- 3. Preprocessing Tools Initialization ---\n",
    "# Initialize the set of standard English stopwords from NLTK.\n",
    "stop = set(stopwords.words('english'))\n",
    "# Initialize the WordNet Lemmatizer to reduce words to their base form.\n",
    "lemma = WordNetLemmatizer()\n",
    "# Define a set of punctuation marks to be excluded.\n",
    "exclude = set(string.punctuation)\n",
    "# Add domain-specific stopwords to the default list.\n",
    "stop.update(['fig', 'table', 'example', 'figure', 'article', 'history', 'abstract', 'keywords', 'research'])\n",
    "\n",
    "# --- 4. Custom Stopwords Function ---\n",
    "# This function reads custom stopwords from a specified file and adds them to the main set.\n",
    "def add_custom_stopwords(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            custom_stopwords = file.read().split()\n",
    "        stop.update(custom_stopwords)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Custom stopwords file not found: {file_path}\")\n",
    "        print(\"Continuing without custom stopwords.\")\n",
    "\n",
    "add_custom_stopwords(custom_stopwords_path)\n",
    "\n",
    "# --- 5. Text Cleaning Function ---\n",
    "# This function applies a series of cleaning and preprocessing steps to a given text.\n",
    "def clean_text(text):\n",
    "    # Standardize the text by converting to lowercase and replacing unwanted characters/patterns.\n",
    "    text = text.lower().replace('\\n', ' ').replace('\\x0c', ' ').replace('  ', ' ').replace('a b s t r a c t', 'abstract ').replace('k e y w o r d s', 'abstract ').replace('2020 Elsevier Ltd. All rights reserved', '').replace('2021 Published by Elsevier Ltd.', '').replace('2021 Elsevier B.V. All rights reserved.', '').replace('2020 Elsevier Ltd. All rights reserved.', '').replace('2021 Elsevier Ltd. All rights reserved.', '').replace('  ', '').replace('  ', '')\n",
    "    \n",
    "    # Remove numbers, decimals, and units (e.g., '1.2km', '100m').\n",
    "    text = re.sub(r'\\b\\d*\\.?\\d+[\\w/]*\\b', '', text)\n",
    "    \n",
    "    # Tokenize the text into words and filter out stopwords.\n",
    "    # The regex pattern finds words with at least two letters.\n",
    "    words = re.findall(r'\\b[a-zA-Z]{2,}\\b|\\b(?<!\\S)r(?!\\S)\\b', text)\n",
    "    words = [word for word in words if word not in stop]\n",
    "    \n",
    "    # Apply lemmatization to reduce words to their base form.\n",
    "    words = [lemma.lemmatize(word) for word in words]\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    # Remove specific special characters, symbols, and patterns like 'et al'.\n",
    "    cleaned_text = re.sub(r'·|[〠ð—©©\u0001©©]|et al|', '', cleaned_text)\n",
    "    \n",
    "    # Remove Greek letters and mathematical symbols.\n",
    "    cleaned_text = re.sub(r'[\\u0370-\\u03FF\\u2200-\\u22FF]+', '', cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "# --- 6. Main Processing Function ---\n",
    "# This function iterates through all '.txt' files in a given directory,\n",
    "# processes each file using `clean_text`, and saves the output to a new directory.\n",
    "def process_and_save_text_files(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        for file in os.listdir(input_dir):\n",
    "            if file.endswith(\".txt\"):\n",
    "                input_path = os.path.join(input_dir, file)\n",
    "                output_path = os.path.join(output_dir, file)\n",
    "\n",
    "                with open(input_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    cleaned_text = clean_text(content)\n",
    "\n",
    "                with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(cleaned_text)\n",
    "                print(f\"[Save Complete] {file} -> {output_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input directory not found. Please check the path: '{input_dir}'.\")\n",
    "\n",
    "# --- 7. Execution ---\n",
    "# Call the main function to start the text processing.\n",
    "process_and_save_text_files(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d47a5",
   "metadata": {},
   "source": [
    "# 3. Text Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# --- 1. Path Configuration ---\n",
    "# Get the current working directory, which is the 'src' folder.\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Move up one level to the project's root directory.\n",
    "base_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Define the path to the 'data' folder from the project's root.\n",
    "data_folder_path = os.path.join(base_dir, 'data')\n",
    "\n",
    "# This is where NLTK will look for its resources, such as 'wordnet'\n",
    "nltk_data_path = os.path.join(data_folder_path, 'nltk_data')\n",
    "\n",
    "# Set the input and output directory paths for processing.\n",
    "input_directory = os.path.join(data_folder_path, 'txt2')\n",
    "output_directory = os.path.join(data_folder_path, 'txt3')\n",
    "\n",
    "# --- 2. NLTK Setup ---\n",
    "# Set the NLTK data path to ensure resources are loaded correctly.\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    print(\"No NLTK resource. Downloading now.\")\n",
    "    nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"No NLTK resource. Downloading now.\")\n",
    "    nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# --- 3. Main Processing Function ---\n",
    "def process_and_save_files_manual_pos(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    This function reads text files from a specified folder,\n",
    "    tokenizes and lemmatizes the words, and saves the cleaned text to a new folder.\n",
    "    It primarily tries to lemmatize words as verbs, then as nouns if no change occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize necessary tools for text processing.\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        # Use RegexpTokenizer to extract only words, ignoring punctuation.\n",
    "        tokenizer = RegexpTokenizer(r'\\b\\w+\\b')\n",
    "    except LookupError:\n",
    "        print(\"Error: Required NLTK resources could not be loaded. Please ensure they are downloaded.\")\n",
    "        return\n",
    "\n",
    "    # Validate that the input folder exists.\n",
    "    if not os.path.isdir(input_folder):\n",
    "        print(f\"Error: The specified input path '{input_folder}' was not found.\")\n",
    "        return\n",
    "\n",
    "    # Create the output directory if it doesn't already exist.\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"Processed files will be stored in '{output_folder}'\")\n",
    "\n",
    "    # Iterate through all files in the input directory.\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_file_path = os.path.join(input_folder, filename)\n",
    "            output_file_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            print(f\"--- Processing file: {filename} ---\")\n",
    "            \n",
    "            try:\n",
    "                with open(input_file_path, 'r', encoding='utf-8') as infile:\n",
    "                    text = infile.read()\n",
    "\n",
    "                    # Tokenize the text into words and convert to lowercase.\n",
    "                    tokens = tokenizer.tokenize(text.lower())\n",
    "                    \n",
    "                    lemmatized_words = []\n",
    "                    for word in tokens:\n",
    "                        # Process only alphabetic words that are not in the stop words list.\n",
    "                        if word.isalpha() and word not in stop_words:\n",
    "                            # 1. Attempt to lemmatize the word as a verb.\n",
    "                            lemmatized_word = lemmatizer.lemmatize(word, pos=wordnet.VERB)\n",
    "                            # 2. If the verb lemmatization doesn't change the word, try lemmatizing as a noun.\n",
    "                            if lemmatized_word == word:\n",
    "                                lemmatized_word = lemmatizer.lemmatize(word, pos=wordnet.NOUN)\n",
    "                            lemmatized_words.append(lemmatized_word)\n",
    "\n",
    "                    processed_text = ' '.join(lemmatized_words)\n",
    "                \n",
    "                with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "                    outfile.write(processed_text)\n",
    "                    \n",
    "                print(f\"Success: '{filename}' has been processed and saved.\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file '{filename}': {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_and_save_files_manual_pos(input_directory, output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
